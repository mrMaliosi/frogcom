# Руководство разработчика FrogCom

## Обзор проекта

FrogCom - это REST API для генерации текста с использованием различных LLM моделей через vLLM. Проект предоставляет гибкий интерфейс для взаимодействия с языковыми моделями и управления их конфигурацией.

## Архитектура

Проект построен по принципу многослойной архитектуры (Layered Architecture) с четким разделением ответственности:

```
┌─────────────────────────────────────┐
│              API Layer              │  ← Эндпоинты и маршруты
├─────────────────────────────────────┤
│           Service Layer             │  ← Бизнес-логика
├─────────────────────────────────────┤
│            Model Layer              │  ← Модели данных (Pydantic)
├─────────────────────────────────────┤
│         Configuration Layer         │  ← Конфигурация приложения
└─────────────────────────────────────┘
```

### Структура проекта

```
src/frogcom/
├── __init__.py              # Экспорты пакета
├── main.py                  # Точка входа приложения
├── app.py                   # Создание FastAPI приложения
├── config.py                # Конфигурация приложения
├── models.py                # Pydantic модели данных
├── generator.py             # Устаревший генератор (для совместимости)
├── llm_instance.py          # Устаревший экземпляр LLM (для совместимости)
├── server.py                # Устаревший сервер (для совместимости)
├── api/                     # API слой
│   ├── __init__.py
│   ├── routes.py            # Маршруты API
│   └── middleware.py        # Middleware для логирования
└── services/                # Сервисный слой
    ├── __init__.py
    ├── llm_service.py       # Сервис для работы с LLM
    ├── logging_service.py   # Сервис логирования
    └── prompt_service.py    # Сервис обработки промптов
```

## Слои архитектуры

### 1. Configuration Layer (`config.py`)

Отвечает за управление конфигурацией приложения:

- **`AppConfig`** - основная конфигурация
- **`LLMConfig`** - конфигурация LLM модели
- **`LoggingConfig`** - конфигурация логирования
- **`APIConfig`** - конфигурация API сервера

Конфигурация может загружаться из переменных окружения через метод `AppConfig.from_env()`.

### 2. Model Layer (`models.py`)

Содержит Pydantic модели для валидации данных:

- **`GenerateRequest`** - запрос на генерацию текста
- **`GenerateResponse`** - ответ с сгенерированным текстом
- **`LLMConfigRequest`** - запрос на изменение конфигурации LLM
- **`LLMConfigResponse`** - ответ с текущей конфигурацией
- **`Message`** - модель сообщения в чате
- **`ErrorResponse`** - модель ошибки
- **`HealthResponse`** - модель ответа health check

### 3. Service Layer (`services/`)

Содержит бизнес-логику приложения:

#### `LLMService`
- Управление LLM моделями
- Генерация текста
- Динамическое изменение конфигурации
- Потокобезопасность

#### `LoggingService`
- Логирование запросов и ответов
- Ротация лог файлов
- Обработка ошибок

#### `PromptService`
- Извлечение промптов из различных форматов
- Валидация сообщений
- Форматирование для отображения

### 4. API Layer (`api/`)

Содержит эндпоинты и middleware:

#### `routes.py`
- **`/health`** - проверка здоровья сервиса
- **`/generate`** - генерация текста
- **`GET /config/llm`** - получение конфигурации LLM
- **`PUT /config/llm`** - обновление конфигурации LLM

#### `middleware.py`
- Логирование HTTP запросов и ответов
- Обработка CORS

## API Эндпоинты

### 1. Health Check
```http
GET /health
```

Возвращает статус сервиса и информацию о загруженной модели.

### 2. Генерация текста
```http
POST /generate
Content-Type: application/json

{
  "prompt": "Привет, как дела?",
  "max_tokens": 100,
  "temperature": 0.7
}
```

Или с сообщениями:
```http
POST /generate
Content-Type: application/json

{
  "messages": [
    {"role": "user", "content": "Привет, как дела?"}
  ],
  "max_tokens": 100
}
```

### 3. Получение конфигурации LLM
```http
GET /config/llm
```

### 4. Обновление конфигурации LLM
```http
PUT /config/llm
Content-Type: application/json

{
  "model_name": "facebook/opt-350m",
  "temperature": 0.8,
  "max_tokens": 512
}
```

## Переменные окружения

| Переменная | Описание | По умолчанию |
|------------|----------|--------------|
| `LLM_MODEL` | Название LLM модели | `facebook/opt-125m` |
| `GPU_MEMORY_UTILIZATION` | Использование GPU памяти | `0.5` |
| `MAX_TOKENS` | Максимальное количество токенов | `256` |
| `TEMPERATURE` | Температура генерации | `0.7` |
| `TOP_P` | Top-p параметр | `0.9` |
| `API_HOST` | Хост API сервера | `0.0.0.0` |
| `API_PORT` | Порт API сервера | `8888` |
| `LOG_DIR` | Директория для логов | `logs` |
| `MAX_LOG_SIZE_MB` | Максимальный размер лог файла | `100` |

## Разработка

### Установка зависимостей
```bash
uv sync
```

### Запуск в режиме разработки
```bash
uv run python -m frogcom.main
```

### Проверка типов
```bash
uv run mypy src/frogcom/
```

### Проверка стиля кода
```bash
uv run ruff check src/frogcom/
```

### Автоформатирование
```bash
uv run ruff format src/frogcom/
```

## Стандарты кодирования

### 1. Именование
- **Файлы и модули**: snake_case (`llm_service.py`)
- **Классы**: PascalCase (`LLMService`)
- **Функции и переменные**: snake_case (`generate_text`)
- **Константы**: UPPER_SNAKE_CASE (`MAX_LOG_SIZE_MB`)

### 2. Документация
- Все модули должны содержать docstring с описанием назначения
- Все классы и публичные методы должны иметь docstring
- Используйте type hints для всех параметров и возвращаемых значений

### 3. Обработка ошибок
- Используйте специфичные исключения
- Логируйте ошибки с контекстом
- Возвращайте понятные сообщения об ошибках в API

### 4. Тестирование
- Покрывайте тестами критическую бизнес-логику
- Используйте pytest для тестирования
- Мокайте внешние зависимости

## Расширение функциональности

### Добавление нового эндпоинта

1. Добавьте модель запроса/ответа в `models.py`
2. Добавьте бизнес-логику в соответствующий сервис
3. Создайте метод в `APIRoutes` в `api/routes.py`
4. Зарегистрируйте маршрут в `_setup_routes()`

### Добавление нового сервиса

1. Создайте файл в `services/`
2. Реализуйте класс сервиса с четкой ответственностью
3. Добавьте dependency injection в `app.py`
4. Обновите `__init__.py` для экспорта

### Изменение конфигурации

1. Добавьте новые поля в соответствующий класс конфигурации
2. Обновите метод `from_env()` для поддержки переменных окружения
3. Обновите документацию

## Мониторинг и логирование

### Логи
- Все запросы и ответы логируются автоматически
- Ошибки логируются с полным контекстом
- Логи ротируются по размеру и времени

### Health Check
- Эндпоинт `/health` предоставляет информацию о состоянии сервиса
- Проверяет загруженность модели
- Возвращает версию API

## Безопасность

### Рекомендации для продакшена
1. Ограничьте CORS origins
2. Добавьте аутентификацию и авторизацию
3. Используйте HTTPS
4. Ограничьте размер запросов
5. Добавьте rate limiting
6. Настройте мониторинг и алерты

## Производительность

### Оптимизация
1. Используйте connection pooling для базы данных
2. Кэшируйте часто используемые данные
3. Настройте правильные параметры vLLM
4. Мониторьте использование GPU памяти

### Масштабирование
1. Используйте load balancer для горизонтального масштабирования
2. Рассмотрите использование Redis для кэширования
3. Настройте мониторинг метрик

## Troubleshooting

### Частые проблемы

1. **Ошибка инициализации LLM**
   - Проверьте доступность GPU
   - Убедитесь в корректности названия модели
   - Проверьте доступную память

2. **Медленная генерация**
   - Уменьшите `max_tokens`
   - Увеличьте `gpu_memory_utilization`
   - Проверьте загрузку GPU

3. **Ошибки логирования**
   - Проверьте права доступа к директории логов
   - Убедитесь в наличии свободного места

## Контрибьюция

1. Создайте feature branch
2. Следуйте стандартам кодирования
3. Добавьте тесты для новой функциональности
4. Обновите документацию
5. Создайте pull request

## Лицензия

Проект использует лицензию [укажите лицензию].
